# -*- coding: utf-8 -*-
"""CS 287 T4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OFx4fHW-ijRatwvPqpukwhpTY9M3T9iK

# HW 4 - All About Attention

Welcome to CS 287 HW4. To begin this assignment first turn on the Python 3 and GPU backend for this Colab by clicking `Runtime > Change Runtime Type` above.

In this homework you will be reproducing the decomposable attention model in Parikh et al. https://aclweb.org/anthology/D16-1244. (This is one of the models that inspired development of the transformer). 



## Goal

We ask that you finish the following goals in PyTorch:

1. Implement the vanilla decomposable attention model as described in that paper.
2. Implement the decomposable attention model with intra attention or another extension.
3. Visualize the attentions in the above two parts.
4. Implement a mixture of models with uniform prior and perform training with exact log marginal likelihood (see below for detailed instructions)
5. Train the mixture of models in part 4 with VAE. (This may not produce a better model, this is still a research area) 
6. Interpret which component specializes at which type of tasks using the posterior.

Consult the paper for model architecture and hyperparameters, but you are also allowed to tune the hyperparameters yourself.

## Setup

This notebook provides a working definition of the setup of the problem itself. You may construct your models inline or use an external setup (preferred) to build your system.
"""

!pip install -q torch torchtext opt_einsum git+https://github.com/harvardnlp/namedtensor

import torch
# Text text processing library and methods for pretrained word embeddings
import torchtext
from torchtext.vocab import Vectors, GloVe

# Named Tensor wrappers
from namedtensor import ntorch, NamedTensor
from namedtensor.text import NamedField

"""The dataset we will use of this problem is known as the Stanford Natural Language Inference (SNLI) Corpus ( https://nlp.stanford.edu/projects/snli/ ). It is collection of 570k English sentence pairs with relationships entailment, contradiction, or neutral, supporting the task of natural language inference (NLI).

To start, `torchtext` requires that we define a mapping from the raw text data to featurized indices. These fields make it easy to map back and forth between readable data and math, which helps for debugging.
"""

# Our input $x$
TEXT = NamedField(names=('seqlen',))

# Our labels $y$
LABEL = NamedField(sequential=False, names=())

"""Next we input our data. Here we will use the standard SNLI train split, and tell it the fields."""

train, val, test = torchtext.datasets.SNLI.splits(
    TEXT, LABEL)

"""Let's look at this data. It's still in its original form, we can see that each example consists of a premise, a hypothesis and a label."""

print('len(train)', len(train))
print('vars(train[0])', vars(train[0]))

"""In order to map this data to features, we need to assign an index to each word an label. The function build vocab allows us to do this and provides useful options that we will need in future assignments."""

TEXT.build_vocab(train)
LABEL.build_vocab(train)
print('len(TEXT.vocab)', len(TEXT.vocab))
print('LABEL.vocab', LABEL.vocab)

"""Finally we are ready to create batches of our training data that can be used for training and validating the model. This function produces 3 iterators that will let us go through the train, val and test data."""

train_iter, val_iter, test_iter = torchtext.data.BucketIterator.splits(
    (train, val, test), batch_size=16, device=torch.device("cuda"), repeat=False)

"""Let's look at a single batch from one of these iterators."""

batch = next(iter(train_iter))
print("Size of premise batch:", batch.premise.shape)
print("Size of hypothesis batch:", batch.hypothesis.shape)
premise = batch.premise.get("batch", 1)
print("Second premise in batch", premise)
print("Converted back to string:", " ".join([TEXT.vocab.itos[i] for i in premise.tolist()]))
hypothesis = batch.hypothesis.get("batch", 1)
print("Second hypothesis in batch", hypothesis)
print("Converted back to string:", " ".join([TEXT.vocab.itos[i] for i in hypothesis.tolist()]))

"""Similarly it produces a vector for each of the labels in the batch."""

print("Size of label batch:", batch.label.shape)
example = batch.label.get("batch", 1)
print("Second in batch", example.item())
print("Converted back to string:", LABEL.vocab.itos[example.item()])

"""Finally the Vocab object can be used to map pretrained word vectors to the indices in the vocabulary."""

# Build the vocabulary with word embeddings
# Out-of-vocabulary (OOV) words are hashed to one of 100 random embeddings each
# initialized to mean 0 and standarad deviation 1 (Sec 5.1)
import random
unk_vectors = [torch.randn(300) for _ in range(100)]
TEXT.vocab.load_vectors(vectors='glove.6B.300d',
                        unk_init=lambda x:random.choice(unk_vectors))
# normalized to have l_2 norm of 1
vectors = TEXT.vocab.vectors
vectors = vectors / vectors.norm(dim=1,keepdim=True)
vectors = NamedTensor(vectors, ('word', 'embedding'))
TEXT.vocab.vectors = vectors
print("Word embeddings shape:", TEXT.vocab.vectors.shape)
print("Word embedding of 'follows', first 10 dim ",
      TEXT.vocab.vectors.get('word', TEXT.vocab.stoi['follows']) \
                        .narrow('embedding', 0, 10))

"""## Assignment

Now it is your turn to implement the models described at the top of the assignment using the data given by this iterator.

### Instructions for latent variable mixture model.

For the last part of this assignment we will consider a latent variable version of this model. This is a use of latent variable as a form of ensembling.

Instead of a single model, we use $K$ models $p(y | \mathbf{a}, \mathbf{b}; \theta_k)$ ($k=1,\cdots,K$), where $K$ is a hyperparameter. Let's introduce a discrete latent variable $c\sim \text{Uniform}(1,\cdots, K)$ denoting which model is being used to produce the label $y$, then the marginal likelihood is


$$
p(y|\mathbf{a}, \mathbf{b}; \theta) = \sum_{c=1}^K p(c) p(y | \mathbf{a}, \mathbf{b}; \theta_c)
$$

When $K$ is small, we can *enumerate* all possible values of $c$ to maximize the log marginal likelihood. 

We can also use variational auto encoding to perform efficient training. We first introduce an inference network $q(c| y, \mathbf{a}, \mathbf{b})$, and the ELBO is

$$
\log p(y|\mathbf{a}, \mathbf{b}; \theta)  \ge \mathbb{E}_{c \sim q(c|y, \mathbf{a}, \mathbf{b})} \log p(y|\mathbf{a},\mathbf{b}; \theta_c) - KL(q(c|y, \mathbf{a}, \mathbf{b})|| p(c)),
$$

where $p(c)$ is the prior uniform distribution. We can calculate the $KL$ term in closed form, but for the first term in ELBO, due to the discreteness of $c$, we cannot use the reparameterization trick. Instead we use REINFORCE to estimate the gradients (or see slides):

$$
\nabla \mathbb{E}_{c \sim q(c|y, \mathbf{a}, \mathbf{b})} \log p(y|\mathbf{a},\mathbf{b}; \theta_c) = \mathbb{E}_{c \sim q(c|y, \mathbf{a}, \mathbf{b})} \left [\nabla \log p(y|\mathbf{a},\mathbf{b}; \theta_c) + \log p(y|\mathbf{a},\mathbf{b}; \theta_c)  \nabla \log q(c|y, \mathbf{a}, \mathbf{b})\right]
$$


At inference time, to get $p(y|\mathbf{a}, \mathbf{b}; \theta)$ we use enumeration to calculate it exactly. For posterior inference, we can either use $q(c| y, \mathbf{a}, \mathbf{b})$ to approximate the true posterior or use Bayes rule to calculate the posterior exactly.

To interpret what specialized knowledge each component $c$ learns, we can find those examples whose posterior reaches maximum at $c$.

When a model is trained, use the following test function to produce predictions, and then upload your best result to the kaggle competition:  https://www.kaggle.com/c/harvard-cs287-s19-hw4
"""

def test_code(model):
    "All models should be able to be run with following command."
    model.eval()
    upload = []
    # Update: for kaggle the bucket iterator needs to have batch_size 10
    test_iter = torchtext.data.BucketIterator(test, train=False, batch_size=10, device=torch.device("cuda"))
    for batch in test_iter:
        # Your prediction data here (don't cheat!)
        probs = model(batch.premise, batch.hypothesis)
        # here we assume that the name for dimension classes is `classes`
        _, argmax = probs.max('classes')
        upload += argmax.tolist()

    with open("predictions.txt", "w") as f:
        f.write("Id,Category\n")
        for i, u in enumerate(upload):
            f.write(str(i) + "," + str(u) + "\n")

"""In addition, you should put up a (short) write-up following the template provided in the repository:  https://github.com/harvard-ml-courses/nlp-template

## Additional Goodies
"""

import os.path

import matplotlib.pyplot as plt
import numpy as np

from google.colab import drive
drive.mount('/content/drive')

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
path = "/content/drive/My Drive/College/current classes/CS 287 spring 2019/Homework/HW4/"

embedding_dim = 300
nclasses = len(LABEL.vocab)

def train_model(model, train_iter, optimizer, criterion, every=1000, key='', epoch=0, best_val_loss=1E9):
    model.train()
    total_loss=0.
    num_batches=0.
    total=0.
    total_right=0.
    for b, batch in enumerate(train_iter):
        optimizer.zero_grad()
        output = model(batch.premise, batch.hypothesis)
        preds = output.max('output')[1]
        preds_eq = preds.eq(batch.label)
        loss = 0.
        loss += criterion(output, batch.label).values
        loss.backward()
        optimizer.step()
        num_batches += 1
        total += float(batch.premise.shape['batch'])
        total_right += preds_eq.sum().item()
        total_loss += loss.detach().item()*float(batch.premise.shape['batch'])
        torch.cuda.empty_cache() 
        if(b%every == 0):
            print('[B{:4d}] Train Loss: {:.3e}'.format(b, total_loss/total))
            if key != '':
                torch.save({
                'epoch': epoch,
                'model': model.state_dict(),
                'optimizer': optimizer.state_dict(),
                'best_val_loss': best_val_loss
                }, path+'models'+key+'E{:4d}B{:4d}'.format(epoch, b))
    return total_right / total, total_loss / total

def test_model(model, test_iter, criterion, num_batches=-1):
    model.eval()
    with torch.no_grad():
        total = 0
        total_right = 0
        total_loss = 0
        for b, batch in enumerate(test_iter):
            if(not(num_batches==-1) and b > num_batches):
                break
            output = model(batch.premise, batch.hypothesis)
            preds = output.max('output')[1]
            preds_eq = preds.eq(batch.label)
            loss = 0.
            loss += criterion(output, batch.label).values
            total += float(batch.premise.shape['batch'])
            total_right += preds_eq.sum().item()
            total_loss += loss.detach().item()*float(batch.premise.shape['batch'])
    return total_right / total, total_loss / total

def checkpoint_trainer(model, optimizer, criterion, key, version, nepochs=20):
    # model with lowest validation loss thus far is saved at path+'models'+key
    # we can also load a specific version, i.e. path+'models'+key+'E20B2000'
    best_val_loss = -1
    epoch_start = 0

    # load checkpoint
    fname = path+'models'+key+version
    if os.path.isfile(fname):
        checkpoint = torch.load(fname)
        epoch_start = checkpoint['epoch'] + 1
        model.load_state_dict(checkpoint['model'])
        optimizer.load_state_dict(checkpoint['optimizer'])
        best_val_loss = checkpoint['best_val_loss']
        print("Loaded Checkpoint:", epoch_start, best_val_loss, np.exp(best_val_loss))

    for epoch in range(epoch_start, nepochs):
        train_acc, train_loss = train_model(model, train_iter, 
                                            optimizer,
                                            criterion,
                                            every=1000,
                                            epoch=epoch,
                                            best_val_loss=best_val_loss)
        val_acc, val_loss = test_model(model, val_iter, criterion)
        print('[E{:4d}] | Train Acc: {:.3e} Train Loss: {:.3e} | Val Acc: {:.3e} Val Loss: {:.3e} PPL: {:.3e}'.format(epoch, train_acc, train_loss, val_acc, val_loss, np.exp(val_loss)))
        if(val_loss < best_val_loss or best_val_loss == -1):
            best_val_loss = val_loss
            torch.save({
                'epoch': epoch,
                'model': model.state_dict(),
                'optimizer': optimizer.state_dict(),
                'best_val_loss': best_val_loss,
                }, path+'models'+key)

"""##  1. Implement the vanilla decomposable attention model as described in that paper.

### Paper

#### Definitions

$\mathbf{a}, \mathbf{b}$ input sentences, where each word represented as a word embedding vector. Each sentence is prepended by a "NULL" token.
\begin{align*}
  \mathbf{a} &= (a_1, \dots, a_{l_a}), \\
  \mathbf{b} &= (b_1, \dots, b_{l_b}), \\
  a_i, b_j &\in \mathbb{R}^d.
\end{align*}
vanilla case:
\begin{align*}
  \bar{\mathbf{a}} &:= \mathbf{a}, \\
  \bar{\mathbf{b}} &:= \mathbf{b}.
\end{align*}

#### Attend

\begin{align*}
  e_{ij} &:= F'(\bar{a}_i, \bar{b}_j) := F(\bar{a}_i)^\intercal F(\bar{b}_j),
\end{align*}
$F$ feed-forward NN with ReLU activation.
Attention weights then normalized with $\alpha_j$ and $\beta_i$ softly aligned
to $\bar{b}_j$ and $\bar{a}_i$ respectively.
\begin{align*}
  \beta_i &:= \sum_{j=1}^{l_b} \frac{\exp(e_{ij})}{\sum_{k=1}^{l_b}\exp(e_{ik})}\bar{b}_j, \\
  \alpha_j &:= \sum_{i=1}^{l_b} \frac{\exp(e_{ij})}{\sum_{k=1}^{l_b}\exp(e_{kj})}\bar{a}_j.
\end{align*}

#### Compare

Compare aligned phrases $\{(\bar{a}_i,\beta_i)\}_{i=1}^{l_a}$ and
$\{(\bar{b}_j,\alpha_j)\}_{j=1}^{l_b}$ using a function $G$, where $G$ is a
feed-forward NN.
\begin{align*}
  \mathbf{v}_{1,i} &:= G([\bar{a}_i,\beta_i]) \quad \forall i\in[1,\dots,l_a], \\
  \mathbf{v}_{2,j} &:= G([\bar{b}_j,\alpha_j]) \quad \forall j\in[1,\dots,l_b].
\end{align*}

#### Aggregate

Two sets of comparison vectors $\{\mathbf{v}_{1,i}\}_{i=1}^{l_a}$ and
$\{\mathbf{v}_{2,j}\}_{j=1}^{l_b}$. Aggregate over each set by summation:
\begin{align*}
  \mathbf{v}_1 &= \sum_{i=1}^{l_a} \mathbf{v}_{1,i}, \\
  \mathbf{v}_2 &= \sum_{j=1}^{l_b} \mathbf{v}_{2,j}.
\end{align*}
$H$ is a feed-forward NN:
$$ \hat{\mathbf{y}} = H([\mathbf{v}_1, \mathbf{v}_2]) \in \mathbb{R}^C. $$

#### Training and Evaluation

$$ \hat{y} = \underset{i}{\operatorname{argmax}} \hat{\mathbf{y}}_i. $$
Multi-class cross-entropy loss with dropout regularization:
$$ L(\theta_F, \theta_G, \theta_H) = \frac{1}{N} \sum_{n=1}^N \sum_{c=1}^C y_c^{(n)} \log \frac{\exp(\hat{y}_c)}{\sum_{c'=1}^C \exp(\hat{y}_{c'})}. $$

#### Hyperparameters

the paper:
-  Adagrad for optimization with the default initial accumulator value of 0.1.
- Dropout regularization was used for all ReLU layers, but not for the finallinear layer. 
- network size (2-layers, each with 200 neurons), batch size (4), dropout ratio (0.2) and learning rate (0.05–vanilla,  0.025–intra-attention)

we'll use batch size 16

### Code

#### Class Definitions
"""

class FeedForwardReLU(ntorch.nn.Module):
    def __init__(self, input_name, input_dim, output_name, output_dim=200, hidden_dim=200, dropout=0.2):
        super(FeedForwardReLU, self).__init__()
        self.input_name = input_name
        self.output_name = output_name
        self.lineara = ntorch.nn.Linear(input_dim, hidden_dim).spec(input_name, 'hidden')
        self.dropouta = ntorch.nn.Dropout(dropout)
        self.linearb = ntorch.nn.Linear(hidden_dim, output_dim).spec('hidden', output_name)
        self.dropoutb = ntorch.nn.Dropout(dropout)
    def forward(self, x):
        x = self.lineara(x).relu()
        x = self.dropouta(x)
        x = self.linearb(x).relu()
        x = self.dropoutb(x)
        return x

class DecomposableAttentionCore(ntorch.nn.Module):
    def __init__(self, representation_dim=embedding_dim, hidden_dim=200):
        super(DecomposableAttentionCore, self).__init__()
        self.F = FeedForwardReLU('representation', representation_dim, 'attention', hidden_dim)
        self.G = FeedForwardReLU('representation', representation_dim * 2, 'comparison', hidden_dim)
        self.H1 = FeedForwardReLU('comparison', hidden_dim * 2, 'hidden', hidden_dim)
        self.H2 = ntorch.nn.Linear(hidden_dim, nclasses).spec('hidden', 'output')
        # attention visualization
        self.attnWeightsAlpha = None
        self.attnWeightsBeta = None
    def forward(self, abar, bbar):
        # abar: batch, premiseseqlen, representation
        # bbar: batch, premiseseqlen, representation
        adecomposed = self.F(abar) # batch, premiseseqlen, attention
        bdecomposed = self.F(bbar) # batch, hypothesisseqlen, attention
        e = adecomposed.dot('attention', bdecomposed) # batch, premiseseqlen, hypothesisseqlen
        self.attnWeightsAlpha = e.softmax('premiseseqlen') # batch, premiseseqlen, hypothesisseqlen
        self.attnWeightsBeta = e.softmax('hypothesisseqlen') # batch, premiseseqlen, hypothesisseqlen
        alpha = self.attnWeightsAlpha.dot('premiseseqlen', abar) # batch, hypothesisseqlen, representation
        beta = self.attnWeightsBeta.dot('hypothesisseqlen', bbar) # batch, premisesseqlen, representation
        aBeta = ntorch.cat((abar, beta), 'representation') # batch, premisesseqlen, representation * 2
        bAlpha = ntorch.cat((bbar, alpha), 'representation') # batch, hypothesisseqlen, representation * 2
        v1dot = self.G(aBeta) # batch, premiseseqlen, comparison
        v2dot = self.G(bAlpha) # batch, hypothesisseqlen, comparison
        v1 = v1dot.sum('premiseseqlen') # batch, comparison
        v2 = v2dot.sum('hypothesisseqlen') # batch, comparison
        v = ntorch.cat((v1, v2), 'comparison') # batch, comparison * 2
        yhat = self.H2(self.H1(v)) # batch, output
        return yhat

class DecomposableAttentionVanilla(ntorch.nn.Module):
    def __init__(self, embedding_dim=embedding_dim, hidden_dim=200):
        super(DecomposableAttentionVanilla, self).__init__()
        self.embedding = ntorch.nn.Embedding(len(TEXT.vocab), embedding_dim).spec('seqlen', 'representation')
        self.core = DecomposableAttentionCore(embedding_dim, hidden_dim)
        # attention visualization
        self.attnWeightsAlpha = None
        self.attnWeightsBeta = None
    def forward(self, a, b):
        # a: batch, seqlen
        # b: batch, seqlen
        abar = self.embedding(a).rename('seqlen', 'premiseseqlen') # batch, premiseseqlen, embedding
        bbar = self.embedding(b).rename('seqlen', 'hypothesisseqlen') # batch, hypothesisseqlen, embedding
        yhat = self.core(abar, bbar) # batch, output
        self.attnWeightsAlpha = self.core.attnWeightsAlpha
        self.attnWeightsBeta = self.core.attnWeightsBeta
        return yhat

"""#### Training and Testing"""

model = DecomposableAttentionVanilla().to(device)
# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
optimizer = torch.optim.Adagrad(model.parameters(), lr=0.05, lr_decay=0, weight_decay=0, initial_accumulator_value=0.1)
criterion = ntorch.nn.CrossEntropyLoss().spec("output")

# model with lowest validation loss thus far is saved at path+'models'+key
# we can also load a specific version, i.e. path+'models'+key+'E20B2000'
key = '4.1.vanilla.prototype'
version = ''

checkpoint_trainer(model, optimizer, criterion, key, version)

"""#### Scratch"""

# val_acc, val_loss = test_model(model, val_iter, criterion)
# print('[E{:4d}] | Train Acc: {:.3e} Train Loss: {:.3e} | Val Acc: {:.3e} Val Loss: {:.3e} PPL: {:.3e}'.format(0, 0, 0, val_acc, val_loss, np.exp(val_loss)))
# best_val_loss = val_loss
# torch.save({
#     'epoch': 0,
#     'model': model.state_dict(),
#     'optimizer': optimizer.state_dict(),
#     'best_val_loss': best_val_loss,
#     }, path+'models'+key)

"""##    2. Implement the decomposable attention model with intra attention or another extension.

### Paper

#### Definitions

$$ f_{ij}:=F_{intra}(a_i)^\intercal F_{intra}(a_j), $$
$F_{intra}$ feed-forward NN.
Self-aligned phrases:
$$ a_i' := \sum_{j=1}^{l_a} \frac{\exp(f_{ij}+d_{i-j})}{\sum_{k=1}^{l_a} \exp(f_{ik}+d_{i-k})} a_j. $$
Distance biases $d_{i-j} \in \mathbb{R}$ are such that for $|i-j|>10$ the biases are all the same. Then $\bar{a}_i := [a_i, a_i']$ and likewise for $\bar{b}_i$.

### Code

#### Class Definitions
"""

class DecomposableAttentionIntra(ntorch.nn.Module):
    def __init__(self, embedding_dim=embedding_dim, hidden_dim=200):
        super(DecomposableAttentionVanilla, self).__init__()
        self.embedding = ntorch.nn.Embedding(len(TEXT.vocab), embedding_dim).spec('seqlen', 'representation')
        self.Fintra = FeedForwardReLU('representation', embedding_dim, 'hidden', hidden_dim)
        self.core = DecomposableAttentionCore(embedding_dim * 2, hidden_dim)
        # attention visualization
        self.aselfAttnWeights = None
        self.bselfAttnWeights = None
    def forward(self, a, b):
        # a: batch, seqlen
        # b: batch, seqlen
        aembedding = self.embedding(a).rename('seqlen', 'premiseseqlen') # batch, premiseseqlen, embedding
        bembedding = self.embedding(b).rename('seqlen', 'hypothesisseqlen') # batch, hypothesisseqlen, embedding
        adecomposedintra = self.Fintra(aembedding) # batch, premiseseqlen, hidden
        bdecomposedintra = self.Fintra(bembedding) # batch, hypothesisseqlen, hidden
        # the following renaming dance is necessary to distinguish between two otherwise equivalent dimensions in a square matrix.
        adecomposedintra = adecomposedintra.rename('premiseseqlen', 'premiseseqlen2') # batch, premiseseqlen2, hidden
        bdecomposedintra = bdecomposedintra.rename('hypothesisseqlen', 'hypothesisseqlen2') # batch, hypothesisseqlen2, hidden
        af = adecomposedintra.dot('hidden', adecomposedintra) # batch, premiseseqlen, premiseseqlen2
        bf = bdecomposedintra.dot('hidden', bdecomposedintra) # batch, hypothesisseqlen, hypothesisseqlen2
        ## TODO: distance biases
        self.aselfAttnWeights = af.softmax('premiseseqlen', af) # batch, premiseseqlen, premiseseqlen2
        self.bselfAttnWeights = bf.softmax('hypothesisseqlen', bf) # batch, hypothesisseqlen, hypothesisseqlen2
        aprime = self.aselfAttnWeights.dot('premiseseqlen', aembedding) # batch, premiseseqlen2, embedding
        bprime = self.bselfAttnWeights.dot('hypothesisseqlen', bembedding) # batch, hypothesisseqlen2, embedding
        aprime = aprime.rename('premiseseqlen2', 'premiseseqlen') # batch, premiseseqlen, embedding
        bprime = aprime.rename('hypothesisseqlen2', 'premiseseqlen') # batch, hypothesisseqlen, embedding
        abar = ntorch.cat((aembedding, af), 'representation') # batch, premiseseqlen, embedding * 2
        bbar = ntorch.cat((bembedding, bf), 'representation') # batch, hypothesisseqlen, embedding * 2
        yhat = self.core(abar, bbar) # batch, output
        self.attnWeightsAlpha = core.attnWeightsAlpha
        self.attnWeightsBeta = core.attnWeightsBeta
        return yhat

"""#### Training and Testing"""

model = DecomposableAttentionIntra().to(device)
# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
optimizer = torch.optim.Adagrad(model.parameters(), lr=0.025, lr_decay=0, weight_decay=0, initial_accumulator_value=0.1)
criterion = ntorch.nn.CrossEntropyLoss().spec("output")

# model with lowest validation loss thus far is saved at path+'models'+key
# we can also load a specific version, i.e. path+'models'+key+'E20B2000'
key = '4.2.intra.prototype'
version = ''

checkpoint_trainer(model, optimizer, criterion, key, version)

"""##  3. Visualize the attentions in the above two parts."""

def visualize(model, batch):
    model.eval()
    # a: batch, seqlen
    # b: batch, seqlen
    intra = isinstance(model, DecomposableAttentionIntra)
    yhat = model(batch.premise, batch.hypothesis)
    for batchnum in range(batch.premise.shape['batch']):
        premise = batch.premise.get("batch", batchnum)
        hypothesis = batch.hypothesis.get("batch", batchnum)
        label = batch.label.get("batch", batchnum)
        attnWeightsAlpha = model.attnWeightsAlpha.get("batch", batchnum) # premiseseqlen, hypothesisseqlen
        attnWeightsBeta = model.attnWeightsBeta.get("batch", batchnum) # premiseseqlen, hypothesisseqlen
        if intra:
            aselfAttnWeights = model.aselfAttnWeights.get("batch", batchnum) # premiseseqlen, premiseseqlen
            bselfAttnWeights = model.bselfAttnWeights.get("batch", batchnum) # hypothesisseqlen, hypothesisseqlen
        
        premise = [TEXT.vocab.itos[i] for i in premise.tolist()]
        hypothesis = [TEXT.vocab.itos[i] for i in hypothesis.tolist()]
        label = LABEL.vocab.itos[label.item()]
        
        # title, ylabel, ytickslabels, xlabel, xticklabels, data
        graph_info = []
        graph_info.append(('$\\alpha$ Attention', 'premise', premise, 'hypothesis', hypothesis, attnWeightsAlpha.cpu().detach().numpy()))
        graph_info.append(('$\\beta$ Attention', 'premise', premise, 'hypothesis', hypothesis, attnWeightsBeta.cpu().detach().numpy()))
        if intra:
            graph_info.append(('$a$ Self-Attention', 'premise', premise, 'premise', premise, aselfAttnWeights.cpu().detach().numpy()))
            graph_info.append(('$b$ Self-Attention', 'hypothesis', hypothesis, 'hypothesis', hypothesis, bselfAttnWeights.cpu().detach().numpy()))
        
        for title, ylabel, row_labels, xlabel, column_labels, data in graph_info:
            fig, ax = plt.subplots()
            heatmap = ax.pcolor(data, cmap=plt.cm.Blues)

            # put the major ticks at the middle of each cell
            ax.set_xticks(np.arange(data.shape[1]) + 0.5)
            ax.set_yticks(np.arange(data.shape[0]) + 0.5)

            # want a more natural, table-like display
            ax.invert_yaxis()
            ax.xaxis.tick_top()
            plt.xticks(rotation=60)

            ax.set_title(title)
            ax.set_xticklabels(column_labels)
            ax.set_xlabel(xlabel)
            ax.set_yticklabels(row_labels)
            ax.set_ylabel(ylabel)

            plt.show()

batch = next(iter(train_iter))
visualize(model, batch)

"""## 4. Implement a mixture of models with uniform prior and perform training with exact log marginal likelihood (see below for detailed instructions)"""



"""## 5. Train the mixture of models in part 4 with VAE. (This may not produce a better model, this is still a research area)"""



"""## 6. Interpret which component specializes at which type of tasks using the posterior."""

